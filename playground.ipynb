{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./helpers')\n",
    "sys.path.append('./optimizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom functions and classes\n",
    "from pymoo_optimizer import PymooOptimizer\n",
    "import benchmark as Benchmark\n",
    "import data_provider as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_val, y_train, y_test, y_val = dp.get_train_test_validation_data('breast', 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = PymooOptimizer(x_train, x_val, y_train, y_val)\n",
    "solution, score = optimizer.optimize(64, 100, Benchmark.dtree_accuracy, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, decision tree accuracy changed from 88.95% to 89.3%\n",
      "\n",
      "Optimal contains 83.3% less columns than the original dataset,reducing from 30 to 5\n",
      "\n",
      "Selected columns are: texture1, concavity1, concave_points1, concavity2, compactness3\n"
     ]
    }
   ],
   "source": [
    "raw_score = np.round(Benchmark.dtree_accuracy(x_train, x_test, y_train, y_test) * 100, 2)\n",
    "optimized_score = np.round(Benchmark.dtree_accuracy(x_train.loc[:, solution], x_test.loc[:, solution], y_train, y_test) * 100, 2)\n",
    "print(f'After optimization, decision tree accuracy changed from {raw_score}% to {optimized_score}%\\n\\n' +\n",
    "        f'Optimal contains {np.round((x_train.shape[1] - np.sum(solution)) / x_train.shape[1] * 100, 1)}% less columns than the original dataset,' +\n",
    "        f'reducing from {x_train.shape[1]} to {np.sum(solution)}\\n\\nSelected columns are: {\", \".join(x_train.columns[solution].tolist())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates multiple solutions, returning the best one and the average score\n",
    "# If the average score is worse than previous, it means that there probably was no true optimization\n",
    "def get_average_heuristics(optimizer, heuristics, tries = 1):\n",
    "    scores = []\n",
    "    solutions = []\n",
    "    for _ in range(tries):\n",
    "        solution, score = optimizer.optimize(64, 50, heuristics, verbose = False)\n",
    "        solutions.append(solution)\n",
    "        scores.append(score)\n",
    "    return solutions[scores.index(np.max(score))], np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tstd       \tmin     \tmax     \n",
      "0  \t50    \t0.883699\t0.00336996\t0.876692\t0.890226\n",
      "1  \t31    \t0.884291\t0.00353212\t0.876692\t0.892481\n",
      "2  \t37    \t0.884361\t0.00413283\t0.876692\t0.896491\n",
      "3  \t27    \t0.886586\t0.00398748\t0.876692\t0.896491\n",
      "4  \t29    \t0.88594 \t0.00514884\t0.874436\t0.896491\n",
      "5  \t33    \t0.887398\t0.00683729\t0.878947\t0.920551\n",
      "6  \t35    \t0.886932\t0.00795525\t0.876692\t0.920551\n",
      "7  \t33    \t0.88783 \t0.00745422\t0.878947\t0.905764\n",
      "8  \t28    \t0.891454\t0.00925382\t0.878947\t0.92005 \n",
      "9  \t34    \t0.891494\t0.00995393\t0.878947\t0.92005 \n",
      "10 \t34    \t0.891353\t0.0104901 \t0.876692\t0.92005 \n",
      "11 \t29    \t0.892466\t0.0126996 \t0.878947\t0.92005 \n",
      "12 \t30    \t0.895479\t0.0141236 \t0.874436\t0.92406 \n",
      "13 \t27    \t0.901935\t0.0135275 \t0.878947\t0.92406 \n",
      "14 \t32    \t0.897454\t0.0139553 \t0.876692\t0.92406 \n",
      "15 \t31    \t0.897554\t0.0131554 \t0.878446\t0.92406 \n",
      "16 \t25    \t0.898972\t0.0138097 \t0.876692\t0.92005 \n",
      "17 \t31    \t0.897253\t0.0139073 \t0.878947\t0.92005 \n",
      "18 \t37    \t0.896762\t0.0144168 \t0.874436\t0.92005 \n",
      "19 \t31    \t0.895754\t0.014456  \t0.874436\t0.92005 \n",
      "20 \t33    \t0.896366\t0.0145187 \t0.874436\t0.92005 \n",
      "21 \t29    \t0.900065\t0.0135863 \t0.876692\t0.922807\n",
      "22 \t32    \t0.897404\t0.0145901 \t0.876692\t0.922807\n",
      "23 \t31    \t0.895719\t0.0143911 \t0.876692\t0.922807\n",
      "24 \t28    \t0.90014 \t0.0139435 \t0.876692\t0.92406 \n",
      "25 \t26    \t0.900892\t0.0138556 \t0.874436\t0.92406 \n",
      "26 \t27    \t0.900511\t0.0144879 \t0.874436\t0.92406 \n",
      "27 \t24    \t0.901293\t0.0143263 \t0.878947\t0.92406 \n",
      "28 \t38    \t0.897449\t0.0151587 \t0.876692\t0.924561\n",
      "29 \t21    \t0.904005\t0.0155879 \t0.878947\t0.924561\n",
      "30 \t21    \t0.906201\t0.0148678 \t0.880451\t0.924561\n",
      "31 \t29    \t0.904556\t0.0163614 \t0.878947\t0.924561\n",
      "32 \t33    \t0.903093\t0.0152675 \t0.87594 \t0.924561\n",
      "33 \t35    \t0.899634\t0.0154344 \t0.878947\t0.924561\n",
      "34 \t30    \t0.902942\t0.0145748 \t0.876692\t0.924561\n",
      "35 \t31    \t0.902045\t0.0145327 \t0.881203\t0.924561\n",
      "36 \t35    \t0.904201\t0.0139405 \t0.878947\t0.924561\n",
      "37 \t25    \t0.907248\t0.0133634 \t0.875689\t0.934586\n",
      "38 \t27    \t0.905779\t0.0153388 \t0.876692\t0.934586\n",
      "39 \t21    \t0.909308\t0.0152431 \t0.878947\t0.934586\n",
      "40 \t25    \t0.909614\t0.016039  \t0.881203\t0.934586\n",
      "41 \t39    \t0.901023\t0.0157579 \t0.878947\t0.934586\n",
      "42 \t26    \t0.906476\t0.0160076 \t0.881203\t0.934586\n",
      "43 \t26    \t0.907569\t0.0174249 \t0.878947\t0.934586\n",
      "44 \t29    \t0.903273\t0.018467  \t0.878947\t0.934586\n",
      "45 \t36    \t0.902246\t0.016349  \t0.874185\t0.934586\n",
      "46 \t32    \t0.903449\t0.015141  \t0.874436\t0.934586\n",
      "47 \t22    \t0.90591 \t0.0167377 \t0.876692\t0.934586\n",
      "48 \t31    \t0.90409 \t0.0169884 \t0.878947\t0.934586\n",
      "49 \t31    \t0.906922\t0.0146405 \t0.883459\t0.934586\n",
      "50 \t34    \t0.901223\t0.0175313 \t0.876692\t0.934586\n",
      "After feature creation, decision tree accuracy set to 89.47%\n",
      "\n",
      "After feature creation and selection, decision tree accuracy set to 89.47%\n",
      "\n",
      "Optimal contains 48.4% less columns than the original dataset,reducing from 31 to 16\n",
      "\n",
      "Selected columns are: smoothness1, compactness1, concave_points1, symmetry1, perimeter2, smoothness2, concavity2, concave_points2, fractal_dimension2, texture3, smoothness3, compactness3, concave_points3, symmetry3, fractal_dimension3, evolved_feature\n"
     ]
    }
   ],
   "source": [
    "# Evolve a new feature\n",
    "x_train_2 = x_train\n",
    "x_test_2 = x_test\n",
    "x_val_2 = x_val\n",
    "best_feature_func = optimizer.evolve_new_feature(epochs=50, heuristics=Benchmark.dtree_accuracy)\n",
    "\n",
    "# Apply the evolved feature to the training and test sets\n",
    "x_train_2[\"evolved_feature\"] = x_train_2.apply(lambda row: best_feature_func(*row), axis=1)\n",
    "x_test_2[\"evolved_feature\"] = x_test_2.apply(lambda row: best_feature_func(*row), axis=1)\n",
    "x_val_2[\"evolved_feature\"] = x_val_2.apply(lambda row: best_feature_func(*row), axis=1)\n",
    "\n",
    "# Evaluate the model with the new feature added\n",
    "score_with_new_feature = np.round(Benchmark.dtree_accuracy(x_train_2, x_test_2, y_train, y_test) * 100, 2)\n",
    "optimizer_2 = PymooOptimizer(x_train_2, x_val_2, y_train, y_val)\n",
    "solution, score = optimizer_2.optimize(64, 100, Benchmark.dtree_accuracy, verbose = False)\n",
    "optimized_score_with_new_feature = np.round(Benchmark.dtree_accuracy(x_train_2.loc[:, solution], x_test_2.loc[:, solution], y_train, y_test) * 100, 2)\n",
    "print(f'After feature creation, decision tree accuracy set to {score_with_new_feature}%\\n\\n' +\n",
    "      f'After feature creation and selection, decision tree accuracy set to {optimized_score_with_new_feature}%\\n\\n' +\n",
    "        f'Optimal contains {np.round((x_train.shape[1] - np.sum(solution)) / x_train.shape[1] * 100, 1)}% less columns than the original dataset,' +\n",
    "        f'reducing from {x_train.shape[1]} to {np.sum(solution)}\\n\\nSelected columns are: {\", \".join(x_train.columns[solution].tolist())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(columns=[\"evolved_feature\"], inplace=True)\n",
    "x_test.drop(columns=[\"evolved_feature\"], inplace=True)\n",
    "x_val.drop(columns=[\"evolved_feature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.74 1\n",
      "94.74 94.74\n",
      "After feature creation, decision tree accuracy was equal to 94.74%\n",
      "\n",
      "With previous data, it was 89.12%\n",
      "\n",
      "Optimal contains 93.3% less columns than the original dataset,\n"
     ]
    }
   ],
   "source": [
    "# Evolve a new set of features\n",
    "x_train_2 = pd.DataFrame({})\n",
    "x_test_2 = pd.DataFrame({})\n",
    "x_val_2 = pd.DataFrame({})\n",
    "\n",
    "old_accuracy = 0\n",
    "new_accuracy = 1\n",
    "feature_count = 0\n",
    "\n",
    "while new_accuracy - old_accuracy > 0:\n",
    "    old_accuracy = new_accuracy\n",
    "    feature_count += 1\n",
    "    optimizer = PymooOptimizer(pd.concat([x_train, x_train_2], axis=1), pd.concat([x_val, x_val_2], axis=1), y_train, y_val)\n",
    "    best_feature_func = optimizer.evolve_new_feature(epochs=50, heuristics=Benchmark.dtree_accuracy, verbose=False, target_train = x_train_2, target_test = x_val_2)\n",
    "\n",
    "    x_train_2[f'evolved_feature_{feature_count}'] = pd.concat([x_train, x_train_2], axis=1).apply(lambda row: best_feature_func(*row), axis=1)\n",
    "    x_test_2[f'evolved_feature_{feature_count}'] = pd.concat([x_test, x_test_2], axis=1).apply(lambda row: best_feature_func(*row), axis=1)\n",
    "    x_val_2[f'evolved_feature_{feature_count}'] = pd.concat([x_val, x_val_2], axis=1).apply(lambda row: best_feature_func(*row), axis=1)\n",
    "    \n",
    "    new_accuracy = np.round(Benchmark.dtree_accuracy(x_train_2, x_test_2, y_train, y_test) * 100, 2)\n",
    "\n",
    "score = np.round(Benchmark.dtree_accuracy(x_train, x_test, y_train, y_test) * 100, 2)\n",
    "print(f'After feature creation, decision tree accuracy was equal to {new_accuracy}%\\n\\n' +\n",
    "        f'With previous data, it was {score}%\\n\\n' +\n",
    "        f'Optimal contains {100 - np.round(x_train_2.shape[1] / x_train.shape[1] * 100, 1)}% less columns than the original dataset,')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
